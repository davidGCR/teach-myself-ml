{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Back Propagation Revision\n",
    "\n",
    "Understanding _back propagation_ algorithm is really crucial. In this note, I try to implement it to make sure I comprehend the maths behind. This helps me to easily absorb other variations such as _back propagation through time (BPTT))_ in RNN.\n",
    "\n",
    "___Objective:___\n",
    "- Step 1: Compute the __gradient__ by back propogation.\n",
    "- Step 2: Implement __numerical gradient__ and use it to check whether the gradient calculated in step 1 is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. Compute Gradient using Back Propagation\n",
    "\n",
    "Describe Neural Network:\n",
    "\n",
    "- $L = 2$ --> 2 layers (2 hidden layers + 1 output layer)\n",
    "- $N = 10$ --> 10 examples\n",
    "- $D = H_0 = 15$ --> 15 input features\n",
    "- $H_1 = 15$ --> 20 nodes in the 1st hidden layer)\n",
    "- $C = H_2 = 3$ --> 3 node in the output layer, ie. 1 output classes)\n",
    "\n",
    "Let's write some variables:\n",
    "\n",
    "- $W^{(1)} \\in \\mathbf{R}^{D \\times H_1}$\n",
    "- $b^{(1)} \\in \\mathbf{R}^{1 \\times H_1}$\n",
    "- $z_j^{(l)} = W_{j:}^{(l)T} a^{(l-1)} + b_j^{(l)}$\n",
    "- $z^{(l)} = W^{(l)T} a^{(l-1)} + b^{(l)}$. Note: $a^{(0)} = x$\n",
    "- $x_i \\in \\mathbf{R}^{1 \\times D}$ --> $X \\in \\mathbf{R}^{N \\times D}$. Note: we usually use $x_i$ as a row vector in practice.\n",
    "- $y_i \\in \\mathbf{R}^{1 \\times C}$ --> $Y \\in \\mathbf{R}^{N \\times C}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "def my_rand(d1, d2):\n",
    "    return np.random.rand(d1, d2) - 0.5\n",
    "\n",
    "N = 10\n",
    "D = 15\n",
    "H1 = 20\n",
    "C = 3\n",
    "\n",
    "X = my_rand(N, D)\n",
    "W1 = my_rand(D, H1)\n",
    "W2 = my_rand(H1, C)\n",
    "\n",
    "b1 = my_rand(1, H1)\n",
    "b2 = my_rand(1, C)\n",
    "\n",
    "Ws = [W1, W2]\n",
    "bs = [b1, b2]\n",
    "\n",
    "Y_truth = my_rand(N, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# X: input\n",
    "# Ws, bs: Weights and biases. Ws = [W1, W2, W3, ...]. bs = [b1, b2, b3, ...]\n",
    "# activation_function: sigmoid, tanh, ReLU...\n",
    "# Return: \n",
    "#   A = [a1, a2, a3, ...]. A[-1] is the predicted value (Y_predicted)\n",
    "def forward_propagation(X, Ws, bs, activation):\n",
    "    assert(len(Ws) == len(bs))\n",
    "    \n",
    "    Zs = []\n",
    "    As = []\n",
    "    for k in range(len(Ws)):\n",
    "        a_prev = X if k == 0 else As[-1]\n",
    "        zk = np.dot(a_prev, Ws[k]) + bs[k]\n",
    "        ak = activation(zk)\n",
    "        Zs.append(zk)\n",
    "        As.append(ak)\n",
    "    \n",
    "    return As, Zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check sigmoid_gradient by numerical gradient:\n",
      "\n",
      "diff: 3.99633590109e-11 \t--> Passed:  True\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_gradient(x):\n",
    "    z = sigmoid(x)\n",
    "    return z * (1 - z)\n",
    "\n",
    "# Use numerical gradient to check if sigmoid_gradient is correct\n",
    "threshold = 1e-8\n",
    "eps = 1e-6\n",
    "m = 3\n",
    "diff = sigmoid_gradient(m) - (sigmoid(m + eps) - sigmoid(m - eps))/(2*eps)\n",
    "\n",
    "print('Check sigmoid_gradient by numerical gradient:\\n')\n",
    "print('diff:', diff, '\\t--> Passed: ', diff < threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1.2 Back propagation\n",
    "\n",
    "Gradients of cost function with respect to the weights of the $l^{th}$ layer:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\frac{\\partial J}{\\partial w_{ij}^{(l)}} = \\frac{\\partial J}{\\partial z_j^{(l)}}. \\frac{\\partial z_j^{(l)}}{\\partial w_{ij}^{(l)}} = e_j^{(l)} a_i^{(l-1)}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "where\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "e_j^{(l)} &=& \\frac{\\partial J}{\\partial z_j^{(l)}} = \\frac{\\partial J}{\\partial a_j^{(l)}} . \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}} \\\\\n",
    "&=& \\left( \\sum_{k = 1}^{d^{(l+1)}} \\frac{\\partial J}{\\partial z_k^{(l+1)}} .\\frac{\\partial z_k^{(l+1)}}{\\partial a_j^{(l)}} \\right) f’(z_j^{(l)}) \\\\\n",
    " &=&\\left( \\sum_{k = 1}^{d^{(l+1)}} e_k^{(l+1)} w_{jk}^{(l+1)} \\right) f’(z_j^{(l)}) \\\\\n",
    " &=&\\left( w_{j:}^{(l+1)} e^{(l+1)} \\right) f’(z_j^{(l)}) \\\\\n",
    "\\end{eqnarray}$\n",
    "\n",
    "The term $e^{(L)}$ of the output layer depends on the cost function (look at the term $\\frac{\\partial J}{\\partial a_j^{(L)}}$ in it). For example, given the cost function as mean squared error (MSE) and $N = 1$, then \n",
    "$\\begin{eqnarray}\n",
    "e_j^{(L)} = (a_j^{(L)} - y).f'(z_j^{(L)})\n",
    "\\end{eqnarray}$.\n",
    "\n",
    "With the same reasoning, we have: \n",
    "$\\begin{eqnarray}\n",
    "\\frac{\\partial J}{\\partial b_{j}^{(l)}} = e_j^{(l)}\n",
    "\\end{eqnarray}$\n",
    "\n",
    "For batch (mini-batch) gradient descent, visit http://machinelearningcoban.com/2017/02/24/mlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# X: input\n",
    "# Y_truth: labels \n",
    "# Ws: [W1, W2, W3, ...]\n",
    "# bs: [b1, b2, b3, ...]\n",
    "# activation: activation function\n",
    "# activation_gradient: function to calculate the gradient of the activation function\n",
    "# Return:\n",
    "#   Delta_Ws = [Delta_W1, Delta_W2, Delta_W3, ...]\n",
    "#   Delta_bs = [Delta_b1, Delta_b2, Delta_b3, ...]\n",
    "def back_propagation(X, Y_truth, Ws, bs, activation, activation_gradient):\n",
    "    N, C = Y_truth.shape\n",
    "    As, Zs = forward_propagation(X, Ws, bs, activation)\n",
    "    \n",
    "    # Compute Es\n",
    "    Es = []\n",
    "    for k in range(len(Ws))[::-1]:\n",
    "        if k == len(Ws) - 1:\n",
    "            # NOTE: This computation of E_L is based on the assumption that the cost function is SSE\n",
    "            Ek = (As[-1] - Y_truth) / N * activation_gradient(Zs[-1])\n",
    "        else:\n",
    "            Ek = np.dot(Es[-1], Ws[k+1].T) * activation_gradient(Zs[k])\n",
    "        Es.append(Ek)\n",
    "    Es = Es[::-1] # Reverse it\n",
    "    \n",
    "    # Compute Delta_Ws\n",
    "    Delta_Ws = []\n",
    "    for k in range(len(Ws))[::-1]:\n",
    "        a_prev = X if k == 0 else As[k-1]\n",
    "        Delta_Wk = np.dot(a_prev.T, Es[k])\n",
    "        Delta_Ws.append(Delta_Wk)\n",
    "    Delta_Ws = Delta_Ws[::-1]\n",
    "    \n",
    "    # Compute Delta_bs\n",
    "    Delta_bs = []\n",
    "    for k in range(len(bs)):\n",
    "        Delta_bk = Es[k].sum(axis=0).reshape(1, -1)\n",
    "        Delta_bs.append(Delta_bk)\n",
    "    \n",
    "    return Delta_Ws, Delta_bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compare with Numerical Gradient\n",
    "Computing gradient is extremely important but potentially error-prone. To make sure it's correct, we will compare the result with that of numerical gradient.\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "f'(z) \\approx \\frac{f(z+\\varepsilon) - f(z-\\varepsilon)}{2\\varepsilon}\n",
    "\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# X: input\n",
    "# Y_truth: labels\n",
    "# Ws, bs: Weights and biases. Ws = [W1, W2, W3, ...]. bs = [b1, b2, b3, ...]\n",
    "# Return: cost (loss) of the prediction using SSE\n",
    "def cost(X, Y_truth, Ws, bs, activation):\n",
    "    As, _ = forward_propagation(X, Ws, bs, activation)\n",
    "    Y = As[-1]\n",
    "    N, C = Y_truth.shape\n",
    "    return 1/(2*N) * np.sum((Y - Y_truth) ** 2)\n",
    "\n",
    "def neighbourhood(W, i, j, eps):\n",
    "    W_plus = W.copy()\n",
    "    W_minus = W.copy()\n",
    "    W_plus[i][j] += eps\n",
    "    W_minus[i][j] -= eps\n",
    "    return W_plus, W_minus\n",
    "\n",
    "# X: input\n",
    "# Y_truth: labels \n",
    "# Ws, bs: Weights and biases. Ws = [W1, W2, W3, ...]. bs = [b1, b2, b3, ...]\n",
    "# cost: cost function\n",
    "# Return: Delta_Ws, Delta_bs\n",
    "#   Delta_Ws = [Delta_W1, Delta_W2, Delta_W3, ...]\n",
    "#   Delta_bs = [Delta_b1, Delta_b2, Delta_b3, ...]\n",
    "def numerical_gradient(X, Y_truth, Ws, bs, cost, activation):\n",
    "    assert(X.shape[0] == Y_truth.shape[0])\n",
    "    assert(len(Ws) == len(bs))\n",
    "    \n",
    "    eps = 1e-6\n",
    "    Delta_Ws = []\n",
    "    Delta_bs = []\n",
    "    for k in range(len(Ws)):\n",
    "        # Gradients for Ws\n",
    "        Wk = Ws[k]\n",
    "        Delta_Wk = np.zeros_like(Wk)\n",
    "        for i in range(Wk.shape[0]):\n",
    "            for j in range(Wk.shape[1]):\n",
    "                Wk_plus, Wk_minus = neighbourhood(Wk, i, j, eps)\n",
    "                \n",
    "                cost_plus = cost(X, Y_truth, Ws[:k] + [Wk_plus] + Ws[k+1:], bs, activation)\n",
    "                cost_minus = cost(X, Y_truth, Ws[:k] + [Wk_minus] + Ws[k+1:], bs, activation)\n",
    "                Delta_Wk[i][j] = (cost_plus - cost_minus) / (2*eps)\n",
    "                \n",
    "        Delta_Ws.append(Delta_Wk)\n",
    "    \n",
    "        # Gradients for bs\n",
    "        bk = bs[k]\n",
    "        Delta_bk = np.zeros_like(bk)\n",
    "        for i in range(bk.shape[0]):\n",
    "            for j in range(bk.shape[1]):\n",
    "                bk_plus, bk_minus = neighbourhood(bk, i, j, eps)\n",
    "                \n",
    "                cost_plus = cost(X, Y_truth, Ws, bs[:k] + [bk_plus] + bs[k+1:], activation)\n",
    "                cost_minus = cost(X, Y_truth, Ws, bs[:k] + [bk_minus] + bs[k+1:], activation)\n",
    "                Delta_bk[i][j] = (cost_plus - cost_minus) / (2*eps)\n",
    "                \n",
    "        Delta_bs.append(Delta_bk)\n",
    "\n",
    "    return Delta_Ws, Delta_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare with numerical gradient:\n",
      "\n",
      "diff(W1): 1.11115868661e-09 \t--> Passed: True\n",
      "diff(b1): 2.49699209548e-10 \t--> Passed: True\n",
      "diff(W2): 3.55458525023e-10 \t--> Passed: True\n",
      "diff(b2): 5.84080230485e-11 \t--> Passed: True\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "from numpy.linalg import linalg as LA\n",
    "\n",
    "As = forward_propagation(X, Ws, bs, sigmoid)\n",
    "Delta_Ws1, Delta_bs1 = back_propagation(X, Y_truth, Ws, bs, sigmoid, sigmoid_gradient)\n",
    "Delta_Ws2, Delta_bs2 = numerical_gradient(X, Y_truth, Ws, bs, cost, sigmoid)\n",
    "\n",
    "assert(len(Delta_Ws1) == len(Delta_Ws2))\n",
    "assert(len(Delta_bs1) == len(Delta_bs2))\n",
    "\n",
    "print('Compare with numerical gradient:\\n')\n",
    "threshold = 1e-8\n",
    "for k in range(len(Delta_Ws1)):\n",
    "    assert(Delta_Ws1[k].shape == Delta_Ws2[k].shape)\n",
    "    assert(Delta_bs1[k].shape == Delta_bs2[k].shape)\n",
    "    norm_diff_Wk = LA.norm(Delta_Ws1[k] - Delta_Ws2[k])\n",
    "    norm_diff_bk = LA.norm(Delta_bs1[k] - Delta_bs2[k])\n",
    "    print('diff(W%d):' % (k+1), norm_diff_Wk, '\\t--> Passed:', norm_diff_Wk < threshold)\n",
    "    print('diff(b%d):' % (k+1), norm_diff_bk, '\\t--> Passed:', norm_diff_Wk < threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}